{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blanking out words `suit` to `greasy`\n",
      "Augmented audio saved to ../TIMIT/TRAIN/DR1/FCJF0/SA1-aug.wav\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "import audiomentations as A\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "def read_words(path, convert_to_seconds=True, sample_rate=16000):\n",
    "    words = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            if line == '':\n",
    "                break\n",
    "            words.append({\n",
    "                'word': line.split()[-1].strip(),\n",
    "                'start': line.split()[0],\n",
    "                'end': line.split()[1]\n",
    "            })\n",
    "    if convert_to_seconds:\n",
    "        for word in words:\n",
    "            word['start'] = float(word['start']) / sample_rate\n",
    "            word['end'] = float(word['end']) / sample_rate\n",
    "    return words\n",
    "\n",
    "def middle_words_blankout_augment(audio, sample_rate, words, n_words=1):\n",
    "    audio = audio.copy()\n",
    "    middle = len(words) // 2\n",
    "    start_word = words[middle - math.floor(n_words / 2)]\n",
    "    end_word = words[middle + math.ceil(n_words / 2) - 1]\n",
    "    print(f\"Blanking out words `{start_word['word']}` to `{end_word['word']}`\")\n",
    "    start = int(start_word['start'] * sample_rate)\n",
    "    end = int(end_word['end'] * sample_rate)\n",
    "    audio[start:end] = 0\n",
    "    return audio\n",
    "\n",
    "def distort_audio(input_path):\n",
    "    words = read_words(input_path.replace('.WAV', '.WRD'))\n",
    "    \n",
    "    # Load audio file\n",
    "    audio, sr = sf.read(input_path)\n",
    "    \n",
    "    # Apply augmentations\n",
    "    augmented_audio = middle_words_blankout_augment(audio, sr, words, n_words=3)\n",
    "    \n",
    "    # Save augmented audio\n",
    "    output_path = Path(input_path).with_name(f\"{Path(input_path).stem}-aug.wav\")\n",
    "    sf.write(output_path, augmented_audio, samplerate=sr)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Example usage\n",
    "input_path = \"../TIMIT/TRAIN/DR1/FCJF0/SA1.WAV\"\n",
    "output_path = distort_audio(input_path)\n",
    "print(f\"Augmented audio saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blanking out words `one` to `forward`\n",
      "Blanking out words `from` to `and`\n",
      "Blanking out words `meeting` to `now`\n",
      "Blanking out words `to` to `movies`\n",
      "Blanking out words `suit` to `greasy`\n",
      "Blanking out words `carry` to `oily`\n",
      "Blanking out words `permanent` to `their`\n",
      "Blanking out words `equipment` to `proper`\n",
      "Blanking out words `her` to `one`\n",
      "Blanking out words `had` to `mean`\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "folder = Path(\"../TIMIT/TRAIN/DR1/FCJF0/\")\n",
    "for file in folder.glob(\"*.WAV\"):\n",
    "    distort_audio(str(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Processing SI1027.WAV\n",
      "[2/10] Processing SI1657.WAV\n",
      "[3/10] Processing SX307.WAV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen/.local/lib/python3.9/site-packages/audiomentations/core/transforms_interface.py:61: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/10] Processing SX397.WAV\n",
      "[5/10] Processing SA1.WAV\n",
      "[6/10] Processing SA2.WAV\n",
      "[7/10] Processing SX217.WAV\n",
      "[8/10] Processing SX37.WAV\n",
      "[9/10] Processing SI648.WAV\n",
      "[10/10] Processing SX127.WAV\n",
      "Generating visualizations...\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import os\n",
    "import whisper\n",
    "import abc\n",
    "import numpy as np\n",
    "import audiomentations as A\n",
    "from copy import deepcopy\n",
    "\n",
    "def distort_audio(audio):\n",
    "    # Define augmentations\n",
    "    augmentations = A.Compose([\n",
    "        A.AddGaussianNoise(max_amplitude=0.01, p=1),\n",
    "        A.ClippingDistortion(p=1),\n",
    "        A.Gain(p=1),\n",
    "        A.PeakingFilter(p=1)\n",
    "    ])\n",
    "    # Apply augmentations\n",
    "    augmented_audio = augmentations(samples=audio, sample_rate=16000)\n",
    "    return augmented_audio\n",
    "\n",
    "class Augmentation(abc.ABC):\n",
    "\n",
    "    def __call__(self, audio, sample_rate, transcript):\n",
    "        return self.call(audio, sample_rate, transcript)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def call(self, audio, sample_rate, transcript):\n",
    "        pass\n",
    "\n",
    "class MiddleWordsBlankoutAugmentation(Augmentation):\n",
    "\n",
    "    def __init__(self, n_words=1):\n",
    "        self.n_words = n_words\n",
    "\n",
    "    def call(self, audio, sample_rate, transcript):\n",
    "        audio = audio.copy()\n",
    "        middle = len(transcript) // 2\n",
    "        start_word = transcript[middle - math.floor(self.n_words / 2)]\n",
    "        end_word = transcript[middle + math.ceil(self.n_words / 2)]\n",
    "        start = int(start_word['start'] * sample_rate)\n",
    "        end = int(end_word['end'] * sample_rate)\n",
    "        audio[start:end] = 0\n",
    "        augmented_transcript = transcript[:middle - math.floor(self.n_words / 2)] + \\\n",
    "                                 transcript[middle + math.ceil(self.n_words / 2):]\n",
    "        return audio, augmented_transcript\n",
    "\n",
    "class MiddleWordsDistortAugmentation(Augmentation):\n",
    "\n",
    "    def __init__(self, n_words=1):\n",
    "        self.n_words = n_words\n",
    "\n",
    "    def call(self, audio, sample_rate, transcript):\n",
    "        audio = audio.copy()\n",
    "        transcript = deepcopy(transcript)\n",
    "        middle = len(transcript) // 2\n",
    "        start_word = transcript[middle - math.floor(self.n_words / 2)]\n",
    "        end_word = transcript[middle + math.ceil(self.n_words / 2)]\n",
    "        start = int(start_word['start'] * sample_rate)\n",
    "        end = int(end_word['end'] * sample_rate)\n",
    "        for i in range(middle-math.floor(self.n_words / 2), middle+math.ceil(self.n_words / 2)):\n",
    "            transcript[i]['word'] = '#' + transcript[i]['word']\n",
    "        audio[start:end] = distort_audio(audio[start:end])\n",
    "        return audio, transcript\n",
    "\n",
    "\n",
    "def augment_and_transcribe(file_path, model, augmentation=None, gt_transcript=None):\n",
    "    audio, sr = sf.read(file_path)\n",
    "    if augmentation:\n",
    "        audio, gt_transcript = augmentation(audio, sr, gt_transcript)\n",
    "    result = model.transcribe(np.array(audio, dtype=np.float32), word_timestamps=True)\n",
    "    return result['segments'][0]['words'], gt_transcript\n",
    "\n",
    "def plot_transcript(transcript, ax, y_pos, label):\n",
    "    for word_info in transcript:\n",
    "        start = word_info['start']\n",
    "        end = word_info['end']\n",
    "        word = word_info['word']\n",
    "        duration = end - start\n",
    "        rect = patches.Rectangle((start, y_pos), duration, 0.25, edgecolor='black', facecolor='skyblue')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(start + duration / 2, y_pos + 0.2, word, ha='center', va='center', fontsize=8)\n",
    "    ax.text(0, y_pos + 0.2, label, ha='right', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "def visualize_transcripts(transcripts, labels):\n",
    "    fig, ax = plt.subplots(figsize=(15, len(transcripts)))\n",
    "    for i, transcript, label in zip(range(len(transcripts)), transcripts, labels):\n",
    "        plot_transcript(transcript, ax, i / len(transcripts), label)\n",
    "    max_duration = max([t[-1]['end'] for t in transcripts])\n",
    "    ax.set_xlim([0, max_duration])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_title('Transcripts Comparison')\n",
    "    return fig\n",
    "\n",
    "def generate_visualizations(directory_path, augmentation=None, model_name='tiny.en'):\n",
    "    model = whisper.load_model(model_name)\n",
    "    transcripts = []\n",
    "    labels = []\n",
    "    file_names = [n for n in os.listdir(directory_path) if n.endswith('.WAV')]\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        print(f'[{i+1}/{len(file_names)}] Processing {file_name}')\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        wrd_file_path = os.path.join(directory_path, file_name.replace('.WAV', '.WRD'))\n",
    "        words = read_words(wrd_file_path)\n",
    "        result, _ = augment_and_transcribe(file_path, model)\n",
    "        result_aug, transcript_aug = augment_and_transcribe(file_path, model, augmentation, words)\n",
    "        transcripts.append(result)\n",
    "        transcripts.append(words)\n",
    "        transcripts.append(result_aug)\n",
    "        transcripts.append(transcript_aug)\n",
    "        labels.append(file_name + ' (whisper)')\n",
    "        labels.append(file_name.replace('.WAV', '.WRD') + ' (gt)')\n",
    "        labels.append(file_name + ' (whisper augmented)')\n",
    "        labels.append(file_name.replace('.WAV', '.WRD') + ' (gt aug)')\n",
    "    print('Generating visualizations...')\n",
    "    figs = []\n",
    "    for i in range(0, len(transcripts), 4):\n",
    "        fig = visualize_transcripts(transcripts[i:i+4], labels[i:i+4])\n",
    "        figs.append(fig)\n",
    "    with PdfPages(f'transcripts_comparison-{model_name}.pdf') as pdf:\n",
    "        for fig in figs:\n",
    "            pdf.savefig(fig)\n",
    "    plt.close('all')\n",
    "\n",
    "# AUG = MiddleWordsBlankoutAugmentation(n_words=3)\n",
    "AUG = MiddleWordsDistortAugmentation(n_words=3)\n",
    "\n",
    "generate_visualizations('../TIMIT/TRAIN/DR1/FCJF0/', AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen/.local/lib/python3.9/site-packages/audiomentations/core/transforms_interface.py:61: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dir = Path(\"../TIMIT/TRAIN/DR1/FCJF0/\")\n",
    "\n",
    "for file in dir.glob(\"*.WAV\"):\n",
    "    audio, sr = sf.read(file)\n",
    "    words = read_words(str(file).replace('.WAV', '.WRD'))\n",
    "    augmented_audio, augmented_words = AUG(audio, sr, words)\n",
    "    output_path = str(file).replace('.WAV', '-aug.wav')\n",
    "    sf.write(output_path, augmented_audio, samplerate=sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
